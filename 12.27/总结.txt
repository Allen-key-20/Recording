原版训练完之后的数据：
高斯球的总数 : 1092398，约110万
  SSIM :    0.8136274
  PSNR :   21.9115085
  LPIPS:    0.2062684

经过不断试验，之间将边缘检测纳入损失函数中得到的提升
是以提升高斯球总数量为代价的，提高总损失，也提高了每个高斯球的梯度
促使更多的分裂。

本周将原版的学习优化过程分开：
第一段是0-3000步，不提升SH系数，学习下采样图像
想法是先学习低频信息，再学习高频信息，
下采样0.25和0.5都可以，十分贴合原版训练的PSNR折线
并且都稍微削减了一些高斯球的数量，能从110万降低至90万左右

然后是3000-6000步，这时段什么也不做，只提升SH系数

6000-12000步，这时段删减高斯球数量
抛弃了原版的重置透明度操作，改为每次迭代降低0.003
大量删减了高斯球的数量，但图像质量并不下滑，甚至持平
在文件夹0.003的对比图片上可以看出，在12000步时的平均PSNR和原版几乎相等
此外，还减少了相当多的伪影

500到15000步，这是原版的增密和删除阶段
原版只靠透明度删除高斯球比较低效，随意增加损失，促使高斯球快速增长
在trainRec文档里的164行，大幅增加损失函数后，高斯球的数量增加到205万
但图像质量却下滑约0.3，大量的冗余高斯球
每个高斯球的三轴的平均数的绝对值小于某个值时，就删除，
相比原版能降低至80万左右

几个步骤配合使用能降低至70万，图像质量并不下滑，相比原版的110万，
PSNR甚至上升0.14
total points number : 712979
  SSIM :    0.8105144
  PSNR :   22.0558147
  LPIPS:    0.2269014
在内存占用方面从原版的268MB下降至168MB

下一步实践：
对sfm得到的点云做预处理删除离群点
参考如LightGaussian之类的论文，压缩168MB
